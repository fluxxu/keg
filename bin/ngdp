#!/usr/bin/env python
import os, sys; sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # noqa isort:skip
import csv
import json
import re
from concurrent import futures
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from glob import iglob
from io import BytesIO, StringIO
from typing import Optional
from urllib.parse import urlparse

import click
from humanize import naturalsize
from tabulate import tabulate
from tqdm import tqdm

from keg import blte, psv
from keg.archive import ArchiveGroup, ArchiveIndex
from keg.build import BuildManager
from keg.cdn import DEFAULT_CONFIG_PATH, DelegatingCDN, LocalCDN, RemoteCDN
from keg.core.fetcher import Fetcher
from keg.core.keg import Keg
from keg.encoding import EncodingFile
from keg.exceptions import IntegrityVerificationError, NetworkError, NoDataError
from keg.psvresponse import Versions
from keg.remote.cache import CacheableHttpRemote
from keg.utils import partition_hash, verify_data


def looks_like_md5(s: str) -> bool:
	return bool(re.match(r"[0-9a-f]{32}", s))


def _close_all_progress_bars():
	for pb in list(tqdm._instances):
		pb.clear()
		pb.close()


class RemoteParam(click.ParamType):
	name = "remote"

	def convert(self, value, param, ctx):
		return ctx.obj.keg.clean_remote(value)


class App:
	def __init__(self, ngdp_dir: str) -> None:
		self.keg = Keg(ngdp_dir)
		self.force_cdn = ""
		self.table_format = "psql"
		self.progress = True

	@property
	def verify(self):
		return self.keg.config.verify

	def tabulate(self, table, headers=None) -> str:
		if isinstance(table, psv.PSVFile):
			headers = table.header
			table = [
				[cell.replace(" ", "\n") for cell in row] for row in table
			]

		if self.table_format == "json":
			if not isinstance(table, list):
				table = list(table)
			return json.dumps(table, indent="\t")
		elif self.table_format == "csv":
			ret = StringIO()
			writer = csv.writer(ret)
			for row in table:
				writer.writerow(row)
			return ret.getvalue()

		if headers is None:
			headers = []

		return tabulate(table, headers=headers, tablefmt=self.table_format)

	def tqdm(self, *args, **kwargs):
		kwargs.setdefault("disable", not self.progress)
		return tqdm(*args, **kwargs)

	def _choose_cdn(self, cdns) -> RemoteCDN:
		if self.force_cdn:
			url = urlparse(self.force_cdn)
			if not url.scheme or not url.netloc or not url.path:
				raise click.ClickException(f"Invalid CDN url: {self.force_cdn}")

			server = f"{url.scheme}://{url.netloc}"
			path = url.path
			config_path = DEFAULT_CONFIG_PATH
		elif not cdns:
			raise click.ClickException("No CDNs available. Use --cdn to specify one.")
		else:
			available_cdns = ", ".join(cdn.name for cdn in cdns)
			cdns_lookup = {cdn.name.lower(): cdn for cdn in cdns}

			for preferred_cdn in self.keg.config.preferred_cdns:
				cdn_name = preferred_cdn.lower()
				if cdn_name in cdns_lookup:
					cdn = cdns_lookup[cdn_name]
					break
			else:
				cdn = cdns[0]

			tqdm.write(f"Using CDN: {cdn.name} (available: {available_cdns})")
			assert cdn.all_servers
			server = cdn.all_servers[0]
			path = cdn.path
			config_path = cdn.config_path

		return RemoteCDN(server, path, config_path)

	def fetch_stateful_data(self, remote: CacheableHttpRemote):
		bar = self.tqdm(leave=False, total=4, bar_format="{desc}", postfix="")

		# Look up all available CDNs and choose one to use
		bar.set_description_str("Receiving CDN list")
		try:
			cdns = remote.get_cdns()
		except NetworkError:
			raise click.ClickException(f"NGDP repository {remote.remote} not found")

		# Find all available versions
		bar.set_description_str("Receiving version list")
		versions = remote.get_versions()

		bar.set_description_str("Receiving background download metadata")
		try:
			remote.get_bgdl()
		except (NetworkError, NoDataError):
			# bgdl is optional
			pass

		# Update blobs
		blobs = {}
		if remote.supports_blobs:
			bar.set_description_str("Receiving blobs")
			try:
				remote.get_blobs()
			except NetworkError:
				# Optional
				pass

			for blob_name in ("game", "install"):
				bar.set_description_str(f"Receiving {blob_name} blob")
				try:
					blobs[blob_name] = remote.get_blob(blob_name)[0]
				except NetworkError:
					# Blobs are optional
					pass

		bar.close()

		return cdns, versions, blobs

	def fetch_versions(self, versions, blobs, remote_cdn, metadata_only):
		# Versions are often duplicated per region.
		# In order not to do all the work several times, we dedupe first.
		deduped_versions = {
			f"{v.build_config}:{v.cdn_config}:{v.product_config}": v for v in versions
		}
		versions = list(deduped_versions.values())

		item_bar = self.tqdm(unit="", leave=False, bar_format="", postfix="", position=0)
		fetchers = [(
			Fetcher(
				version,
				self.keg.local_cdn,
				remote_cdn,
				self.keg,
				verify=self.verify,
			),
			self.tqdm(
				leave=False,
				unit="",
				desc=f"Version {version.build_config}: Waiting.",
				bar_format="{desc}{n_fmt}/{total_fmt}",
				postfix="",
				position=i + 1,
			)
		) for i, version in enumerate(versions)]

		for fetcher, bar in fetchers:
			build_config_key = fetcher.version.build_config

			for queue in fetcher.fetch_metadata():
				bar.n = 0
				bar.total = len(queue)
				bar.set_description(
					f"Version {fetcher.version.build_config}: Fetching {queue.name}"
				)
				for item in queue.drain():
					item_bar.set_description_str(f"Downloading: {item.key}")
					try:
						item.fetch()
					except NetworkError as e:
						tqdm.write(str(e), sys.stderr)
					item_bar.update()
					bar.update()

				if queue.name == "product config" and not fetcher.version.product_config:
					# Backwards compatibility!
					# ProductConfig essentially replaced the game blob.
					# But for some old repos (eg. s1a), there's no ProductConfig yet.
					# This matters because we need the decryption key name from there.
					# So we override the fetcher's product_config with the game blob.
					game_blob = blobs.get("game", {})
					if game_blob:
						tqdm.write("Using legacy ProductConfig support")
						fetcher.product_config = game_blob

			if fetcher.decryption_key_name and not fetcher.decryption_key:
				tqdm.write(
					f"ERROR: Missing decryption key: {fetcher.decryption_key_name}", sys.stderr
				)

			if metadata_only:
				tqdm.write(f"Version {build_config_key}: Metadata updated.")
				bar.close()
			else:
				bar.n = 0
				bar.total = 0
				bar.bar_format = "{desc}"
				bar.set_description_str(f"Version {build_config_key}: Waiting for data...")

		if not metadata_only:
			for fetcher, bar in fetchers:
				for queue in fetcher.fetch_data():
					bar.bar_format = "{desc}{n_fmt}/{total_fmt}"
					bar.n = 0
					bar.total = len(queue)
					bar.set_description(
						f"Version {fetcher.version.build_config}: Fetching {queue.name}"
					)

					for item in queue.drain():
						item_bar.set_description_str(f"Downloading: {item.key}")
						try:
							item.fetch()
						except NetworkError as e:
							tqdm.write(str(e), sys.stderr)
						bar.update()
						bar.refresh()
						item_bar.update()

				tqdm.write(f"Version {fetcher.version.build_config}: Done.")
				bar.close()

		item_bar.close()


@click.group()
@click.option("--ngdp-dir", default=".ngdp")
@click.option("--cdn")
@click.option("--progress/--no-progress", default=True)
@click.option("--table-format", default="psql")
@click.pass_context
def main(ctx, ngdp_dir, cdn, progress, table_format):
	ctx.obj = App(ngdp_dir)
	ctx.obj.force_cdn = cdn
	ctx.obj.table_format = table_format
	ctx.obj.progress = progress


@main.command()
@click.pass_context
def init(ctx):
	if ctx.obj.keg.initialize():
		click.echo(f"Initialized in {ctx.obj.keg.path}")
	else:
		click.echo(f"Reinitialized in {ctx.obj.keg.path}")


@main.command()
@click.argument("remote", type=RemoteParam())
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def fetch(ctx, remote, metadata_only=True):
	click.echo(f"Fetching {remote}")
	http_remote: CacheableHttpRemote = ctx.obj.keg.get_remote(remote)
	cdns, versions, blobs = ctx.obj.fetch_stateful_data(http_remote)
	remote_cdn = ctx.obj._choose_cdn(cdns)

	ctx.obj.fetch_versions(versions, blobs, remote_cdn, metadata_only)


@main.command("fetch-all")
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def fetch_all(ctx, metadata_only=True):
	for remote in ctx.obj.keg.config.fetchable_remotes:
		ctx.invoke(fetch, remote=remote, metadata_only=metadata_only)


@main.command("force-fetch")
@click.argument("remote", type=RemoteParam())
@click.argument("version-keys", nargs=-1, required=True)
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def force_fetch(ctx, remote, version_keys, metadata_only):
	http_remote: CacheableHttpRemote = ctx.obj.keg.get_remote(remote)
	remote_cdn = ctx.obj._choose_cdn(http_remote.get_cached_cdns())

	for version_key in version_keys:
		psvfile = ctx.obj.keg.state_cache.read_psv("/versions", version_key)
		versions = [Versions(row) for row in psvfile]

		ctx.obj.fetch_versions(versions, {}, remote_cdn, metadata_only)


@main.command("fetch-object")
@click.argument("keys", nargs=-1, required=True)
@click.option("--type", default="data", type=click.Choice(["data", "config"]))
@click.pass_context
def fetch_object(ctx, keys, type):
	cdn = ctx.obj._choose_cdn([])
	bar = ctx.obj.tqdm(unit="file", ncols=0, leave=False, total=len(keys))

	if type == "data":
		for key in keys:
			msg = f"Fetching {key}..."
			bar.set_description(f"Fetching {key}...")
			if cdn.local_cdn.has_data(key):
				bar.write(f"{msg} Up-to-date.")
			else:
				try:
					cdn.download_data(key, verify=ctx.obj.verify)
					bar.write(f"{msg} OK")
				except NetworkError:
					bar.write(f"{msg} Not found.")

			bar.update()

		bar.close()

	elif type == "config":
		raise NotImplementedError()


@main.command("add")
@click.argument("paths", nargs=-1, required=True)
@click.option("--type", required=True, type=click.Choice([
	"archive",
	"cdns",
	"config",
	"data-index",
	"loose-file",
	"patch",
	"patch-index",
	"versions",
]))
@click.option("--remote", type=RemoteParam())
@click.pass_context
def add_object(ctx, paths, type, remote):
	def _ingest_move(path: str, ngdp_path: str) -> None:
		click.echo(f"{path} => {ngdp_path}")
		dirname = os.path.dirname(ngdp_path)
		if not os.path.exists(dirname):
			os.makedirs(dirname)

		if os.path.exists(ngdp_path):
			tqdm.write(f"File already exists: {ngdp_path}. Skipping.")
		else:
			os.rename(path, ngdp_path)

	if type == "loose-file":
		click.echo("Loading known encoding files...")

		results = ctx.obj.keg.db.get_build_configs(remote=remote)

		encodings = []
		encoding_keys = set()
		cdn = ctx.obj.keg.local_cdn
		indexed_keys = set()
		for build_config_key, cdn_config_key in results:
			if not cdn.has_config(build_config_key) or not cdn.has_config(cdn_config_key):
				continue

			build = BuildManager(build_config_key, cdn_config_key, cdn, verify=ctx.obj.verify)

			tqdm.write("Reading archive indices")
			for archive_key in build.cdn_config.archives:
				if cdn.has_index(archive_key):
					archive_index = cdn.get_index(archive_key, verify=ctx.obj.verify)
					for key, _, _ in archive_index.items:
						indexed_keys.add(key)

			encoding_keys.add(build.build_config.encoding.encoding_key)
			try:
				encoding_file = build.get_encoding()
			except FileNotFoundError:
				continue

			if encoding_file:
				encodings.append(encoding_file)

		for path in paths:
			key = os.path.basename(path)
			ngdp_path = os.path.join(ctx.obj.keg.objects_path, "data", partition_hash(key))
			if os.path.exists(ngdp_path):
				tqdm.write(f"File already exists: {ngdp_path}. Skipping.")
				continue

			found = False
			# First, check if the key is a known encoding file
			if key in encoding_keys:
				tqdm.write(f"{key} is an encoding file.")
				found = True
			elif key in indexed_keys:
				# Then, check if the key is present in archives.
				# If it is, we don't want to add it for now.
				tqdm.write(f"{key} appears in an archive. Ignoring.")
				continue
			else:
				# Finally, look for the key in all encoding files
				for encoding_file in encodings:
					if encoding_file.has_encoding_key(key):
						tqdm.write(f"Found {key} in {encoding_file}")
						found = True
						break
				else:
					tqdm.write(f"Cannot find {key} in any known encoding file. Will not add.")

			if found:
				with open(path, "rb") as f:
					blte.verify_blte_data(f, key)
				_ingest_move(path, ngdp_path)

		return

	for path in paths:
		key = os.path.basename(path)
		with open(path, "rb") as f:
			data = f.read()

		if type == "config":
			verify_data("config file", data, key, verify=ctx.obj.verify)
			# Sanity check
			assert data.startswith(b"# ")

			ngdp_path = os.path.join(ctx.obj.keg.objects_path, "config", partition_hash(key))
			_ingest_move(path, ngdp_path)

		elif type in ("cdns", "versions"):
			verify_data(f"{type} response", data, key, verify=ctx.obj.verify)
			if not remote:
				raise click.BadParameter(remote, param_hint="--remote")

			# Sanity check
			if type == "cdns":
				assert data.startswith(b"Name!STRING:0|")
			elif type == "versions":
				assert data.startswith(b"Region!STRING:0|")

			psvfile = psv.loads(data.decode())
			ctx.obj.keg.db.write_psv(psvfile, key, remote, f"/{type}")

			ngdp_path = os.path.join(
				ctx.obj.keg.response_cache_dir, type, partition_hash(key)
			)
			_ingest_move(path, ngdp_path)

		elif type in ("data-index", "patch-index"):
			key = os.path.splitext(key)[0]
			verify_data(type, data[-28:], key, verify=ctx.obj.verify)
			obj_dir = "data" if type == "data-index" else "patch"
			ngdp_path = os.path.join(
				ctx.obj.keg.objects_path, obj_dir, partition_hash(key) + ".index"
			)
			_ingest_move(path, ngdp_path)

		elif type == "patch":
			verify_data("patch file", data, key, verify=ctx.obj.verify)
			assert data.startswith(b"ZBSDIFF1")

			ngdp_path = os.path.join(
				ctx.obj.keg.objects_path, "patch", partition_hash(key)
			)
			_ingest_move(path, ngdp_path)

		elif type == "archive":
			# TODO verification
			if not ctx.obj.keg.local_cdn.has_index(key):
				raise click.ClickException(f"Index for {key} not found")

			ngdp_path = os.path.join(
				ctx.obj.keg.objects_path, "data", partition_hash(key)
			)
			_ingest_move(path, ngdp_path)


@main.group()
@click.pass_context
def remote(ctx):
	# Ensure the remotes key is in the config
	if "remotes" not in ctx.obj.keg.config.config:
		ctx.obj.keg.config.config["remotes"] = {}


@remote.command("add")
@click.argument("remotes", nargs=-1, required=True, type=RemoteParam())
@click.option("--writeable", is_flag=True)
@click.option("--default-fetch/--no-default-fetch", default=True)
@click.pass_context
def add_remote(ctx, remotes, default_fetch, writeable):
	for remote in remotes:
		if remote in ctx.obj.keg.config.remotes:
			raise click.ClickException(f"Remote {remote} already exists")

		ctx.obj.keg.config.add_remote(remote, default_fetch, writeable)


@remote.command("rm")
@click.argument("remotes", nargs=-1, required=True, type=RemoteParam())
@click.pass_context
def remove_remote(ctx, remotes):
	for remote in remotes:
		try:
			ctx.obj.keg.config.remove_remote(remote)
		except KeyError:
			raise click.ClickException(f"No such remote: {remote}")


@remote.command("list")
@click.pass_context
def list_remotes(ctx):
	for remote in ctx.obj.keg.config.remotes:
		print(remote)


@main.command("inspect")
@click.argument("remote", type=RemoteParam())
@click.pass_context
def inspect(ctx, remote):
	results = ctx.obj.keg.db.get_versions(remote=remote)

	if not results:
		click.echo(f"No known data for {remote}")
		return

	click.echo(f"Remote: {remote}\n")
	click.echo(ctx.obj.tabulate(
		results, headers=("Build Config", "Build ID", "Version"),
	))


class InstallDirective:

	def __init__(
		self,
		filename: str,
		key: str,
		archive_group: ArchiveGroup = None,
		cdn: LocalCDN = None,
		encoding_file: Optional[EncodingFile] = None,
		install_dir: str = "",
		tqdm_bar: tqdm = None,
		verify: bool = False,

	):
		self.filename = filename
		self.key = key

		assert archive_group
		assert cdn
		assert install_dir
		assert tqdm_bar

		self.archive_group = archive_group
		self.cdn = cdn
		self.encoding_file = encoding_file
		self.install_dir = install_dir
		self.tqdm_bar = tqdm_bar
		self.verify = verify


def do_install_file(directive: InstallDirective):

	try:
		encoding_key = directive.encoding_file.find_by_content_key(directive.key)
	except KeyError:
		tqdm.write(
			f"WARNING: Cannot find {directive.key} ({directive.filename}). Skipping.",
			sys.stderr
		)

		directive.tqdm_bar.update()
		return

	file_path = os.path.join(directive.install_dir, directive.filename)
	dirname = os.path.dirname(file_path)

	os.makedirs(dirname, exist_ok=True)  # This could happen concurrently

	if directive.cdn.has_data(encoding_key):
		encoded_file = directive.cdn.download_data(
			encoding_key,
			verify=directive.verify
		)

		with encoded_file:
			decoder = blte.BLTEDecoder(encoded_file, encoding_key, verify=directive.verify)
			with open(file_path, "wb") as f:
				decoder.decode_and_write(f)
	elif directive.cdn.has_fragment(encoding_key):
		with directive.cdn.get_fragment(encoding_key) as encoded_file:
			decoder = blte.BLTEDecoder(encoded_file, encoding_key, verify=directive.verify)
			with open(file_path, "wb") as f:
				decoder.decode_and_write(f)
	elif directive.archive_group.has_file(encoding_key):
		try:
			data = directive.archive_group.get_file_by_key(encoding_key)
		except NetworkError as e:
			tqdm.write(
				f"ERROR: {file_path} -- {e} (ekey={encoding_key}, ckey={directive.key})"
			)

			directive.tqdm_bar.update()
			raise

		else:
			with open(file_path, "wb") as f:
				f.write(data)
	else:
		directive.tqdm_bar.update()
		raise click.ClickException(
			f"Cannot install {file_path}: Missing data for {encoding_key}. Try running "
			f"`ngdp fetch {remote}`."
		)

	directive.tqdm_bar.update()


@main.command()
@click.argument("remote", type=RemoteParam())
@click.argument("version")
@click.argument("outdir", default=".")
@click.option("--fetch", is_flag=True)
@click.option("--show-tags", is_flag=True)
@click.option("--tags", multiple=True)
@click.option("--dryrun", "--dry-run", is_flag=True)
@click.option("--only", multiple=True)
@click.option("--root/--no-root", default=False)
@click.pass_context
def install(ctx, remote, version, outdir, fetch, tags, show_tags, dryrun, only, root):
	if fetch:
		http_remote: CacheableHttpRemote = ctx.obj.keg.get_remote(remote)
		cdns, versions, blobs = ctx.obj.fetch_stateful_data(http_remote)
		remote_cdn = ctx.obj._choose_cdn(cdns)
		cdn = DelegatingCDN(
			os.path.join(ctx.obj.keg.path, "objects"),
			os.path.join(ctx.obj.keg.path, "fragments"),
			os.path.join(ctx.obj.keg.path, "armadillo"),
			os.path.join(ctx.obj.keg.path, "tmp"),
			remote_cdn,
		)
	else:
		cdn = ctx.obj.keg.local_cdn

	# version can be a BuildName, BuildID or BuildConfig

	try:
		build_config_key, cdn_config_key = ctx.obj.keg.db.find_version(
			remote=remote, version=version
		)
	except Exception as e:
		raise click.ClickException(str(e))

	click.echo(f"Checking out {build_config_key}...")

	build = BuildManager(
		build_config_key, cdn_config_key, cdn, verify=ctx.obj.verify
	)

	try:
		install_file = build.get_install()
	except FileNotFoundError:
		raise click.ClickException("Install file not available locally. Run fetch?")
	if not install_file:
		raise click.ClickException("Install file not found")

	if show_tags:
		click.echo("Valid tags:\n")
		table = ctx.obj.tabulate(
			[(k, v[0]) for k, v in install_file.tags.items()],
			headers=("Tag", "Type")
		)
		click.secho(table, fg="black", bold=True)
		return

	entries = sorted(install_file.filter_entries(tags))
	for filter in only:
		dir_filter = filter.rstrip("/") + "/"
		entries = [
			entry for entry in entries if
			entry[0] == filter or
			entry[1] == filter or
			entry[0].startswith(dir_filter)
		]

	if root:
		root_key = build.build_config.root
		root_size = -1
		entries.append((
			f"{root_key}.root",
			root_key,
			root_size,
		))

	if dryrun:
		table = ctx.obj.tabulate(
			entries, headers=("Filename", "Digest", "Size")
		)
		click.secho(table, fg="black", bold=True)

	total_size = sum(k[2] for k in entries)
	click.echo(f"Total size: {naturalsize(total_size, binary=True)}")

	num_conflicts = len(entries) - len(set(k[0] for k in entries))
	click.echo(f"{num_conflicts} conflicting files")

	install_dir = os.path.abspath(outdir)
	click.echo(f"Installation directory: {install_dir}")

	encoding_file = build.get_encoding()
	if not encoding_file:
		raise click.ClickException("No encoding file found. Cannot proceed.")

	if dryrun:
		return

	archive_group = build.get_archive_group()

	prev_filename, prev_key = "", ""
	bar = ctx.obj.tqdm(unit="file", ncols=0, leave=False, total=len(entries))

	for filename, key, size in entries:
		if filename == prev_filename:
			# Skips over potential conflicts
			if key != prev_key:
				tqdm.write(f"WARNING: Unresolved conflict for {filename}", sys.stderr)

			bar.update()
			continue

		prev_filename, prev_key = filename, key
		file_path = os.path.join(install_dir, filename)

		if os.path.exists(file_path):
			raise click.ClickException(f"{file_path} already exists. Not overwriting.")

		bar.set_description(f"Installing {filename} ({naturalsize(size)})")

		do_install_file(InstallDirective(
			filename,
			key,
			archive_group=archive_group,
			cdn=cdn,
			encoding_file=encoding_file,
			install_dir=install_dir,
			tqdm_bar=bar,
			verify=ctx.obj.verify
		))

	bar.close()


@main.command("log")
@click.argument("remote", type=RemoteParam())
@click.option("--type", default="versions", type=click.Choice(["versions", "cdns"]))
@click.pass_context
def show_log(ctx, remote, type):
	column = f"/{type}"
	results = ctx.obj.keg.db.get_responses(remote=remote, path=column)

	last = ""
	for digest, timestamp in results:
		if digest == last:
			# Skip contiguous digests (always only show oldest)
			continue
		last = digest
		click.secho(f"{type} {digest}", fg="yellow")
		click.echo("Date: " + datetime.fromtimestamp(timestamp).isoformat() + "Z")
		click.echo(f"URL: {remote + column}\n")
		if ctx.obj.keg.state_cache.exists(type, digest):
			contents = ctx.obj.keg.state_cache.read(type, digest)
			table = ctx.obj.tabulate(psv.loads(contents))
			click.secho(table, fg="black", bold=True)
		else:
			click.secho("(not available)", fg="red")
		click.echo()


@main.command("show")
@click.argument("remote", type=RemoteParam())
@click.argument("object")
@click.pass_context
def show_object(ctx, remote, object):
	if object.lower() == "buildconfig":
		column = "BuildConfig"
	elif object.lower() == "cdnconfig":
		column = "CDNConfig"
	elif object.lower() == "productconfig":
		column = "ProductConfig"
	else:
		raise click.ClickException(f"Unknown object type: {object}")

	cursor = ctx.obj.keg.db.cursor()

	cursor.execute(f"""
		SELECT distinct("{column}")
		FROM versions
		WHERE
			remote = ? AND
			"{column}" != ''
	""", (remote, ))

	results = cursor.fetchall()
	if not results:
		raise click.ClickException(f"No known {column} for {remote}.")

	for res, in results:
		click.echo(f"{column}: {res}")


@main.command("fsck")
@click.option("--delete", is_flag=True)
@click.pass_context
def fsck(ctx, delete):
	def _get_objects(patterns):
		return sorted(
			f for pattern in patterns
			for f in iglob(os.path.join(ctx.obj.keg.path, pattern))
		)

	objects = _get_objects((
		"responses/bgdl/*/*/*",
		"responses/blob/*/*/*/*",
		"responses/blobs/*/*/*",
		"responses/cdns/*/*/*",
		"responses/versions/*/*/*",
		"objects/config/*/*/*",
		"objects/configs/data/*/*/*",
		"objects/data/*/*/*",
		"objects/patch/*/*/*",
	))

	fail = 0
	bar = ctx.obj.tqdm(unit="object", ncols=0, leave=True, total=len(objects))
	deleted_str = " (deleted)" if delete else ""
	for path in objects:
		bar.update()
		base_path = path[len(ctx.obj.keg.path) + 1:]  # Strip the full path to .ngdp
		key = os.path.basename(path)
		bar.set_description(f"Checking objects: {key} ({base_path})")
		if key.endswith(".keg_temp"):
			tqdm.write(f"Dangling file: {path}{deleted_str}", sys.stderr)
			if delete:
				os.remove(path)
			continue

		is_data = base_path.startswith("objects/data/")
		if len(key) != 32 and not (is_data and key.endswith(".index")):
			if delete:
				os.remove(path)
			tqdm.write(f"Unknown file: {path}{deleted_str}", sys.stderr)
			continue

		with open(path, "rb") as f:
			try:
				if is_data:
					if path.endswith(".index"):
						key = key[:-len(".index")]
						ArchiveIndex(f.read(), key, verify=True)
					elif os.path.exists(path + ".index"):
						with open(path + ".index", "rb") as index_file:
							index = ArchiveIndex(index_file.read(), key, verify=False)
							for item_key, item_size, item_offset in index.items:
								f.seek(item_offset)
								blte.verify_blte_data(BytesIO(f.read(item_size)), item_key)

					else:
						blte.verify_blte_data(f, key)
				else:
					verify_data("object on disk", f.read(), key, verify=True)
			except IntegrityVerificationError as e:
				if delete:
					os.remove(path)
				tqdm.write(f"Integrity error: {path}{deleted_str}", sys.stderr)
				tqdm.write(str(e), sys.stderr)
				fail += 1
			except Exception as e:
				import traceback
				tqdm.write(f"Error while verifying {path}", sys.stderr)
				tqdm.write(traceback.format_exc(), sys.stderr)

	bar.set_description("Checking responses: Done.")
	bar.close()

	if fail:
		tqdm.write(f"{fail} bad objects!")
		exit(1)


@main.command("parse-encoding")
@click.argument("paths", nargs=-1)
@click.pass_context
def parse_encoding(ctx, paths):
	for path in paths:
		with open(path, "rb") as f:
			data = f.read()

		if data[:4] == b"BLTE":
			data = blte.loads(data, "", verify=False)

		encoding_file = EncodingFile(data, "", verify=False)

		click.echo(ctx.obj.tabulate(
			encoding_file.encoding_keys,
			headers=("Encoded key", "Encoding spec")
		))

		content_keys = [(row[0], row[1][1], *row[1][0]) for row in encoding_file.content_keys]
		# Handle scenario with more than 1 content key
		extra_columns = max(len(r) for r in content_keys) - 2
		headers = ["Content key", "Size", "Encoded key"]
		headers += ["Encoded key (alt.)"] * extra_columns

		click.echo(ctx.obj.tabulate(content_keys, headers=headers))


@main.group()
def archive():
	pass


@archive.command("list")
@click.option("--remote", type=RemoteParam(), multiple=True)
@click.option("--show-available/--hide-available", default=True)
@click.option("--show-unavailable/--hide-unavailable", default=True)
@click.pass_context
def list_archives(ctx, remote, show_available, show_unavailable):
	cdn = ctx.obj.keg.local_cdn
	cdn_config_keys = ctx.obj.keg.db.get_cdn_configs(remotes=remote)
	archives = set()
	for cdn_config_key in cdn_config_keys:
		try:
			cdn_config = cdn.get_cdn_config(cdn_config_key)
		except FileNotFoundError:
			continue

		archives.update(cdn_config.archives)

	for archive_key in sorted(archives):
		if cdn.has_data(archive_key):
			if show_available:
				click.secho(archive_key, bold=True)
		else:
			if show_unavailable:
				click.secho(archive_key, fg="black", bold=True)


@archive.command("extract")
@click.argument("archive-key")
@click.option("--out-dir", default=".")
@click.option("--extract-blte/--no-extract-blte", default=True)
@click.pass_context
def extract_archive(ctx, archive_key, out_dir, extract_blte):
	cdn = ctx.obj.keg.local_cdn
	if not cdn.has_data(archive_key):
		raise click.ClickException(f"Archive {archive_key} not found.")
	if not cdn.has_index(archive_key):
		raise click.ClickException(f"Archive index {archive_key} not found.")

	out_dir = os.path.abspath(os.path.join(out_dir, archive_key))
	if os.path.exists(out_dir):
		raise click.ClickException(f"Directory {out_dir} already exists")
	os.makedirs(out_dir)

	archive = cdn.get_archive(archive_key)
	archive_index = cdn.get_index(archive_key, verify=ctx.obj.verify)
	items = list(archive_index.items)

	bar = ctx.obj.tqdm(unit="files", ncols=0, leave=False, total=len(items))
	for key, size, offset in items:
		bar.update()
		bar.set_description(f"Extracting {key}")
		if extract_blte:
			file_data = archive.get_file(key, size, offset, verify=ctx.obj.verify)
		else:
			file_data = archive.get_file_data(size, offset)

		with open(os.path.join(out_dir, key), "wb") as f:
			f.write(file_data)

	bar.close()


@archive.command("create")
@click.argument("archive-key")
@click.argument("fragment-paths", nargs=-1)
@click.pass_context
def reconstruct_archive(ctx, archive_key, fragment_paths):
	cdn = ctx.obj.keg.local_cdn
	if cdn.has_data(archive_key):
		raise click.ClickException(f"Archive {archive_key} already exists.")
	if not cdn.has_index(archive_key):
		raise click.ClickException(f"Index for {archive_key} not found.")

	archive_index = cdn.get_index(archive_key, verify=ctx.obj.verify)
	items = list(archive_index.items)
	click.echo(f"Reconstructing archive: {len(items)} items required.")

	# Sort items by offset, so they get written serially without seeking
	items.sort(key=lambda k: k[2])

	errors = 0
	temp_path = f"{archive_key}.keg_temp"
	with open(temp_path, "wb") as archive_file:
		bar = ctx.obj.tqdm(unit="fragment", ncols=0, leave=False, total=len(items))
		for key, size, offset in items:
			bar.update()
			for path in fragment_paths:
				filename = os.path.join(path, key)
				if not os.path.exists(filename):
					continue
				with open(filename, "rb") as f:
					if ctx.obj.verify:
						try:
							blte.verify_blte_data(f, key)
						except blte.BLTEError as e:
							# This is actually okay, we only write the required bytes.
							tqdm.write(f"WARNING: {str(e)}")
						f.seek(0)
					# Make sure we didn't miss some data somehow
					assert archive_file.tell() == offset
					archive_file.write(f.read(size))
				break
			else:
				errors += 1
				tqdm.write(f"ERROR: Could not find fragment: {key}", sys.stderr)
				# Write nulls to skip ahead
				archive_file.write(b"\0" * size)
	bar.close()

	if errors:
		click.echo(
			f"Reconstruction failed: {errors}/{len(items)} bad or missing fragments.",
			sys.stderr
		)
		os.remove(temp_path)
	else:
		os.rename(temp_path, archive_key)
		click.echo(f"Written to {os.path.abspath(archive_key)}")


@archive.command("list-fragments")
@click.argument("archive-keys-or-paths", nargs=-1)
@click.pass_context
def list_archive_fragments(ctx, archive_keys_or_paths):
	def get_actual_path(path):
		if os.path.exists(path):
			return path

		if looks_like_md5(key_or_path):
			real_path = os.path.join(
				ctx.obj.keg.objects_path,
				"data",
				partition_hash(key_or_path) + ".index"
			)
			if os.path.exists(real_path):
				return real_path

	items = []
	for key_or_path in archive_keys_or_paths:
		real_path = get_actual_path(key_or_path)
		if not real_path:
			raise click.ClickException(f"No such file or object: {key_or_path}")

		key = os.path.basename(os.path.splitext(real_path)[0])
		if not looks_like_md5(key):
			click.echo(f"WARNING: Invalid key name {repr(key)}", file=sys.stderr)

		with open(real_path, "rb") as f:
			data = f.read()

		archive_index = ArchiveIndex(data, key, verify=ctx.obj.verify)
		items += list(archive_index.items)

	click.secho(
		ctx.obj.tabulate(items, headers=("Key", "Size", "Offset")),
		fg="black", bold=True
	)


if __name__ == "__main__":
	main()
