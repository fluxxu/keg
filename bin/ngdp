#!/usr/bin/env python
import os, sys; sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # noqa isort:skip
import csv
import json
import re
from datetime import datetime
from glob import iglob
from io import BytesIO, StringIO
from urllib.parse import urlparse

import click
from humanize import naturalsize
from tabulate import tabulate
from tqdm import tqdm

from keg import CacheableHttpRemote, blte, psv
from keg.archive import ArchiveGroup, ArchiveIndex
from keg.build import BuildManager
from keg.cdn import DEFAULT_CONFIG_PATH, CacheableCDNWrapper
from keg.core.fetcher import Fetcher
from keg.core.keg import Keg
from keg.encoding import EncodingFile
from keg.exceptions import (
	ArmadilloKeyNotFound, IntegrityVerificationError, NetworkError
)
from keg.http import Versions
from keg.utils import partition_hash, verify_data


def looks_like_md5(s: str) -> bool:
	return bool(re.match(r"[0-9a-f]{32}", s))


def _close_all_progress_bars():
	for pb in list(tqdm._instances):
		pb.clear()
		pb.close()


class RemoteParam(click.ParamType):
	name = "remote"

	def convert(self, value, param, ctx):
		return ctx.obj.keg.clean_remote(value)


class App:
	def __init__(self, ngdp_dir: str) -> None:
		self.keg = Keg(ngdp_dir)
		self.force_cdn = ""

	@property
	def verify(self):
		return self.keg.config.verify

	def tabulate(self, table, headers=None) -> str:
		if isinstance(table, psv.PSVFile):
			headers = table.header
			table = [
				[cell.replace(" ", "\n") for cell in row] for row in table
			]

		if self.table_format == "json":
			if not isinstance(table, list):
				table = list(table)
			return json.dumps(table, indent="\t")
		elif self.table_format == "csv":
			ret = StringIO()
			writer = csv.writer(ret)
			for row in table:
				writer.writerow(row)
			return ret.getvalue()

		if headers is None:
			headers = []

		return tabulate(table, headers=headers, tablefmt=self.table_format)

	def _choose_cdn(self, cdns) -> CacheableCDNWrapper:
		if self.force_cdn:
			url = urlparse(self.force_cdn)
			if not url.scheme or not url.netloc or not url.path:
				raise click.ClickException(f"Invalid CDN url: {self.force_cdn}")

			server = f"{url.scheme}://{url.netloc}"
			path = url.path
			config_path = DEFAULT_CONFIG_PATH
		elif not cdns:
			raise click.ClickException("No CDNs available. Use --cdn to specify one.")
		else:
			available_cdns = ", ".join(cdn.name for cdn in cdns)
			cdns_lookup = {cdn.name.lower(): cdn for cdn in cdns}

			for preferred_cdn in self.keg.config.preferred_cdns:
				cdn_name = preferred_cdn.lower()
				if cdn_name in cdns_lookup:
					cdn = cdns_lookup[cdn_name]
					break
			else:
				cdn = cdns[0]

			tqdm.write(f"Using CDN: {cdn.name} (available: {available_cdns})")
			assert cdn.all_servers
			server = cdn.all_servers[0]
			path = cdn.path
			config_path = cdn.config_path

		return CacheableCDNWrapper(
			base_dir=self.keg.objects_path,
			armadillo_dir=self.keg.armadillo_dir,
			server=server,
			path=path,
			config_path=config_path,
			fragments_path=self.keg.fragments_path,
		)

	def fetch_stateful_data(self, remote: CacheableHttpRemote):
		bar = tqdm(leave=False, total=4, bar_format="{desc}", postfix="")

		# Look up all available CDNs and choose one to use
		bar.set_description_str("Receiving CDN list")
		try:
			cdns = remote.get_cdns()
		except NetworkError:
			raise click.ClickException(f"NGDP repository {remote.remote} not found")

		cdn = self._choose_cdn(cdns)

		# Find all available versions
		bar.set_description_str("Receiving version list")
		versions = remote.get_versions()

		bar.set_description_str("Receiving background download metadata")
		try:
			remote.get_bgdl()
		except NetworkError:
			# bgdl is optional
			pass

		# Update blobs
		bar.set_description_str("Receiving blobs")
		try:
			remote.get_blobs()
		except NetworkError:
			# Optional
			pass

		blobs = {}
		for blob_name in ("game", "install"):
			bar.set_description_str(f"Receiving {blob_name} blob")
			try:
				blobs[blob_name] = remote.get_blob(blob_name)[0]
			except NetworkError:
				# Blobs are optional
				pass

		bar.close()

		return cdn, versions, blobs

	def fetch_versions(self, versions, blobs, remote_cdn, metadata_only):
		# Versions are often duplicated per region.
		# In order not to do all the work several times, we dedupe first.
		deduped_versions = {
			f"{v.build_config}:{v.cdn_config}:{v.product_config}": v for v in versions
		}
		versions = list(deduped_versions.values())

		item_bar = tqdm(unit="", leave=False, bar_format="", postfix="", position=0)
		fetchers = [(
			Fetcher(version),
			tqdm(
				leave=False,
				unit="",
				desc=f"Version {version.build_config}: Waiting.",
				bar_format="{desc}{n_fmt}/{total_fmt}",
				postfix="",
				position=i + 1,
			)
		) for i, version in enumerate(versions)]

		for fetcher, bar in fetchers:
			build_config_key = fetcher.version.build_config

			try:
				for queue in fetcher.fetch_metadata(self.keg.local_cdn, remote_cdn):
					bar.n = 0
					bar.total = len(queue)
					bar.set_description(
						f"Version {fetcher.version.build_config}: Fetching {queue.name}"
					)
					for item in queue.drain():
						item_bar.set_description_str(f"Downloading: {item.key}")
						try:
							item.fetch()
						except NetworkError as e:
							tqdm.write(str(e), sys.stderr)
						item_bar.update()
						bar.update()

					if queue.name == "product config" and not fetcher.version.product_config:
						# Backwards compatibility!
						# ProductConfig essentially replaced the game blob.
						# But for some old repos (eg. s1a), there's no ProductConfig yet.
						# This matters because we need the decryption key name from there.
						# So we override the fetcher's product_config with the game blob.
						game_blob = blobs.get("game", {})
						if game_blob:
							tqdm.write("Using legacy ProductConfig support")
							fetcher.product_config = game_blob
			except ArmadilloKeyNotFound as e:
				_close_all_progress_bars()
				raise click.ClickException(f"Decryption key not found: {str(e)}")

			# if decryption_key_name and product_key not in decryption_keys:
			# 	tqdm.write(f"Build {version.build_config} ({version.versions_name}) is encrypted")
			# 	tqdm.write(f"Decryption key {repr(decryption_key_name)} required")
			# 	decryption_keys[product_key] = decryption_key_name

			if metadata_only:
				tqdm.write(f"Version {build_config_key}: Metadata updated.")
				bar.close()
			else:
				bar.n = 0
				bar.total = 0
				bar.bar_format = "{desc}"
				bar.set_description_str(f"Version {build_config_key}: Waiting for data...")

		if not metadata_only:
			for fetcher, bar in fetchers:
				for queue in fetcher.fetch_data(self.keg.local_cdn, remote_cdn):
					bar.bar_format = "{desc}{n_fmt}/{total_fmt}"
					bar.n = 0
					bar.total = len(queue)
					bar.set_description(
						f"Version {fetcher.version.build_config}: Fetching {queue.name}"
					)

					for item in queue.drain():
						item_bar.set_description_str(f"Downloading: {item.key}")
						try:
							item.fetch()
						except NetworkError as e:
							tqdm.write(str(e), sys.stderr)
						bar.update()
						bar.refresh()
						item_bar.update()

				tqdm.write(f"Version {fetcher.version.build_config}: Done.")
				bar.close()

		item_bar.close()


@click.group()
@click.option("--ngdp-dir", default=".ngdp")
@click.option("--cdn")
@click.option("--table-format", default="psql")
@click.pass_context
def main(ctx, ngdp_dir, cdn, table_format):
	ctx.obj = App(ngdp_dir)
	ctx.obj.force_cdn = cdn
	ctx.obj.table_format = table_format


@main.command()
@click.pass_context
def init(ctx):
	if ctx.obj.keg.initialize():
		click.echo(f"Initialized in {ctx.obj.keg.path}")
	else:
		click.echo(f"Reinitialized in {ctx.obj.keg.path}")


@main.command()
@click.argument("remote", type=RemoteParam())
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def fetch(ctx, remote, metadata_only=True):
	click.echo(f"Fetching {remote}")
	http_remote: CacheableHttpRemote = ctx.obj.keg.get_remote(remote)
	remote_cdn, versions, blobs = ctx.obj.fetch_stateful_data(http_remote)

	# decryption_keys = {}
	# for product_config, key_name in decryption_key_names.items():
	# 	try:
	# 		decryption_keys[key_name] = ctx.obj.keg.get_decryption_key(key_name)
	# 	except ArmadilloKeyNotFound:
	# 		click.echo(f"Repository is encrypted with unknown key {key_name}. Aborting.")
	# 		return

	ctx.obj.fetch_versions(versions, blobs, remote_cdn, metadata_only)


@main.command("fetch-all")
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def fetch_all(ctx, metadata_only=True):
	for remote in ctx.obj.keg.config.fetchable_remotes:
		ctx.invoke(fetch, remote=remote, metadata_only=metadata_only)


@main.command("force-fetch")
@click.argument("remote", type=RemoteParam())
@click.argument("version-keys", nargs=-1, required=True)
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def force_fetch(ctx, remote, version_keys, metadata_only):
	http_remote: CacheableHttpRemote = ctx.obj.keg.get_remote(remote)
	remote_cdn = ctx.obj._choose_cdn(http_remote.get_cached_cdns())

	for version_key in version_keys:
		psvfile = ctx.obj.keg.state_cache.read_psv("/versions", version_key)
		versions = [Versions(row) for row in psvfile]

		ctx.obj.fetch_versions(versions, {}, remote_cdn, metadata_only)


@main.command("fetch-object")
@click.argument("keys", nargs=-1, required=True)
@click.option("--type", default="data", type=click.Choice(["data", "config"]))
@click.pass_context
def fetch_object(ctx, keys, type):
	cdn = ctx.obj._choose_cdn([])
	bar = tqdm(unit="file", ncols=0, leave=False, total=len(keys))

	if type == "data":
		for key in keys:
			msg = f"Fetching {key}..."
			bar.set_description(f"Fetching {key}...")
			if cdn.local_cdn.has_data(key):
				bar.write(f"{msg} Up-to-date.")
			else:
				try:
					cdn.download_data(key, verify=ctx.obj.verify)
					bar.write(f"{msg} OK")
				except NetworkError:
					bar.write(f"{msg} Not found.")

			bar.update()

		bar.close()

	elif type == "config":
		raise NotImplementedError()


@main.command("add")
@click.argument("paths", nargs=-1, required=True)
@click.option("--type", required=True, type=click.Choice([
	"archive",
	"cdns",
	"config",
	"data-index",
	"loose-file",
	"patch",
	"patch-index",
	"versions",
]))
@click.option("--remote", type=RemoteParam())
@click.pass_context
def add_object(ctx, paths, type, remote):
	def _ingest_move(path: str, ngdp_path: str) -> None:
		click.echo(f"{path} => {ngdp_path}")
		dirname = os.path.dirname(ngdp_path)
		if not os.path.exists(dirname):
			os.makedirs(dirname)

		if os.path.exists(ngdp_path):
			tqdm.write(f"File already exists: {ngdp_path}. Skipping.")
		else:
			os.rename(path, ngdp_path)

	if type == "loose-file":
		click.echo("Loading known encoding files...")

		results = ctx.obj.keg.db.get_build_configs(remote=remote)

		encodings = []
		encoding_keys = set()
		cdn = ctx.obj.keg.local_cdn
		indexed_keys = set()
		for build_config_key, cdn_config_key in results:
			if not cdn.has_config(build_config_key) or not cdn.has_config(cdn_config_key):
				continue

			build = BuildManager(build_config_key, cdn_config_key, cdn, verify=ctx.obj.verify)

			tqdm.write("Reading archive indices")
			for archive_key in build.cdn_config.archives:
				if cdn.has_index(archive_key):
					archive_index = cdn.get_index(archive_key, verify=ctx.obj.verify)
					for key, _, _ in archive_index.items:
						indexed_keys.add(key)

			ckey, ekey = build.build_config.encodings
			encoding_keys.add(ekey)
			try:
				encoding_file = build.get_encoding()
			except FileNotFoundError:
				continue

			if encoding_file:
				encodings.append(encoding_file)

		for path in paths:
			key = os.path.basename(path)
			ngdp_path = os.path.join(ctx.obj.keg.objects_path, "data", partition_hash(key))
			if os.path.exists(ngdp_path):
				tqdm.write(f"File already exists: {ngdp_path}. Skipping.")
				continue

			found = False
			# First, check if the key is a known encoding file
			if key in encoding_keys:
				tqdm.write(f"{key} is an encoding file.")
				found = True
			elif key in indexed_keys:
				# Then, check if the key is present in archives.
				# If it is, we don't want to add it for now.
				tqdm.write(f"{key} appears in an archive. Ignoring.")
				continue
			else:
				# Finally, look for the key in all encoding files
				for encoding_file in encodings:
					if encoding_file.has_encoding_key(key):
						tqdm.write(f"Found {key} in {encoding_file}")
						found = True
						break
				else:
					tqdm.write(f"Cannot find {key} in any known encoding file. Will not add.")

			if found:
				with open(path, "rb") as f:
					blte.verify_blte_data(f, key)
				_ingest_move(path, ngdp_path)

		return

	for path in paths:
		key = os.path.basename(path)
		with open(path, "rb") as f:
			data = f.read()

		if type == "config":
			verify_data("config file", data, key, verify=ctx.obj.verify)
			# Sanity check
			assert data.startswith(b"# ")

			ngdp_path = os.path.join(ctx.obj.keg.objects_path, "config", partition_hash(key))
			_ingest_move(path, ngdp_path)

		elif type in ("cdns", "versions"):
			verify_data(f"{type} response", data, key, verify=ctx.obj.verify)
			if not remote:
				raise click.BadParameter(remote, param_hint="--remote")

			# Sanity check
			if type == "cdns":
				assert data.startswith(b"Name!STRING:0|")
			elif type == "versions":
				assert data.startswith(b"Region!STRING:0|")

			psvfile = psv.loads(data.decode())
			ctx.obj.keg.db.write_psv(psvfile, key, remote, f"/{type}")

			ngdp_path = os.path.join(
				ctx.obj.keg.response_cache_dir, type, partition_hash(key)
			)
			_ingest_move(path, ngdp_path)

		elif type in ("data-index", "patch-index"):
			key = os.path.splitext(key)[0]
			verify_data(type, data[-28:], key, verify=ctx.obj.verify)
			obj_dir = "data" if type == "data-index" else "patch"
			ngdp_path = os.path.join(
				ctx.obj.keg.objects_path, obj_dir, partition_hash(key) + ".index"
			)
			_ingest_move(path, ngdp_path)

		elif type == "patch":
			verify_data("patch file", data, key, verify=ctx.obj.verify)
			assert data.startswith(b"ZBSDIFF1")

			ngdp_path = os.path.join(
				ctx.obj.keg.objects_path, "patch", partition_hash(key)
			)
			_ingest_move(path, ngdp_path)

		elif type == "archive":
			# TODO verification
			if not ctx.obj.keg.local_cdn.has_index(key):
				raise click.ClickException(f"Index for {key} not found")

			ngdp_path = os.path.join(
				ctx.obj.keg.objects_path, "data", partition_hash(key)
			)
			_ingest_move(path, ngdp_path)


@main.group()
@click.pass_context
def remote(ctx):
	# Ensure the remotes key is in the config
	if "remotes" not in ctx.obj.keg.config.config:
		ctx.obj.keg.config.config["remotes"] = {}


@remote.command("add")
@click.argument("remotes", nargs=-1, required=True, type=RemoteParam())
@click.option("--writeable", is_flag=True)
@click.option("--default-fetch/--no-default-fetch", default=True)
@click.pass_context
def add_remote(ctx, remotes, default_fetch, writeable):
	for remote in remotes:
		if remote in ctx.obj.keg.config.remotes:
			raise click.ClickException(f"Remote {remote} already exists")

		ctx.obj.keg.config.add_remote(remote, default_fetch, writeable)


@remote.command("rm")
@click.argument("remotes", nargs=-1, required=True, type=RemoteParam())
@click.pass_context
def remove_remote(ctx, remotes):
	for remote in remotes:
		try:
			ctx.obj.keg.config.remove_remote(remote)
		except KeyError:
			raise click.ClickException(f"No such remote: {remote}")


@remote.command("list")
@click.pass_context
def list_remotes(ctx):
	for remote in ctx.obj.keg.config.remotes:
		print(remote)


@main.command("inspect")
@click.argument("remote", type=RemoteParam())
@click.pass_context
def inspect(ctx, remote):
	results = ctx.obj.keg.db.get_versions(remote=remote)

	if not results:
		click.echo(f"No known data for {remote}")
		return

	click.echo(f"Remote: {remote}\n")
	click.echo(ctx.obj.tabulate(
		results, headers=("Build Config", "Build ID", "Version"),
	))


@main.command()
@click.argument("remote", type=RemoteParam())
@click.argument("version")
@click.argument("outdir", default=".")
@click.option("--show-tags", is_flag=True)
@click.option("--tags", multiple=True)
@click.option("--dry-run", "--dryrun", is_flag=True)
@click.option("--only", multiple=True)
@click.option("--root/--no-root", default=False)
@click.pass_context
def install(ctx, remote, version, outdir, tags, show_tags, dryrun, only, root):
	# version can be a BuildName, BuildID or BuildConfig
	try:
		build_config_key, cdn_config_key = ctx.obj.keg.db.find_version(
			remote=remote, version=version
		)
	except Exception as e:
		raise click.ClickException(str(e))

	click.echo(f"Checking out {build_config_key}...")

	http_remote = ctx.obj.keg.get_remote(remote)
	cdn = ctx.obj._choose_cdn(http_remote.get_cached_cdns())

	build = BuildManager(build_config_key, cdn_config_key, cdn, verify=ctx.obj.verify)

	install_file = build.get_install()
	if not install_file:
		raise click.ClickException("Install file not found")

	if show_tags:
		click.echo("Valid tags:\n")
		table = ctx.obj.tabulate(
			[(k, v[0]) for k, v in install_file.tags.items()],
			headers=("Tag", "Type")
		)
		click.secho(table, fg="black", bold=True)
		return

	entries = sorted(install_file.filter_entries(tags))
	for filter in only:
		dir_filter = filter.rstrip("/") + "/"
		entries = [
			entry for entry in entries if
			entry[0] == filter or
			entry[1] == filter or
			entry[0].startswith(dir_filter)
		]

	if root:
		root_key = build.build_config.root
		root_size = -1
		entries.append((
			f"{root_key}.root",
			root_key,
			root_size,
		))

	if dryrun:
		table = ctx.obj.tabulate(
			entries, headers=("Filename", "Digest", "Size")
		)
		click.secho(table, fg="black", bold=True)

	total_size = sum(k[2] for k in entries)
	click.echo(f"Total size: {naturalsize(total_size, binary=True)}")

	num_conflicts = len(entries) - len(set(k[0] for k in entries))
	click.echo(f"{num_conflicts} conflicting files")

	install_dir = os.path.abspath(outdir)
	click.echo(f"Installation directory: {install_dir}")

	encoding_file = build.get_encoding()
	if not encoding_file:
		raise click.ClickException("No encoding file found. Cannot proceed.")

	cdn_config = cdn.get_cdn_config(cdn_config_key, verify=ctx.obj.verify)
	archive_group = ArchiveGroup(
		cdn_config.archives,
		cdn_config.archive_group,
		cdn,
		verify=ctx.obj.verify
	)

	if dryrun:
		return

	prev_filename, prev_key = "", ""
	bar = tqdm(unit="file", ncols=0, leave=False, total=len(entries))
	errors = 0
	for filename, key, size in entries:
		bar.update()
		if filename == prev_filename:
			# Skips over potential conflicts
			if key != prev_key:
				tqdm.write(f"WARNING: Unresolved conflict for {filename}", sys.stderr)
			continue

		prev_filename, prev_key = filename, key
		file_path = os.path.join(install_dir, filename)

		if os.path.exists(file_path):
			raise click.ClickException(f"{file_path} already exists. Not overwriting.")

		bar.set_description(f"Installing {filename} ({naturalsize(size)})")

		try:
			encoding_key = encoding_file.find_by_content_key(key)
		except KeyError:
			tqdm.write(f"WARNING: Cannot find {key} ({filename}). Skipping.", sys.stderr)
			continue

		dirname = os.path.dirname(file_path)
		if not os.path.exists(dirname):
			os.makedirs(dirname)

		if cdn.local_cdn.has_data(encoding_key):
			with cdn.download_data(encoding_key, verify=ctx.obj.verify) as encoded_file:
				decoder = blte.BLTEDecoder(encoded_file, encoding_key, verify=ctx.obj.verify)
				with open(file_path, "wb") as f:
					decoder.decode_and_write(f)
		elif cdn.local_cdn.has_fragment(encoding_key):
			with cdn.local_cdn.get_fragment(encoding_key) as encoded_file:
				decoder = blte.BLTEDecoder(encoded_file, encoding_key, verify=ctx.obj.verify)
				with open(file_path, "wb") as f:
					decoder.decode_and_write(f)
		elif archive_group.has_file(encoding_key):
			try:
				data = archive_group.get_file_by_key(encoding_key)
			except NetworkError as e:
				errors += 1
				tqdm.write(f"ERROR: {file_path} -- {e} (ekey={encoding_key}, ckey={key})")
			else:
				with open(file_path, "wb") as f:
					f.write(data)
		else:
			raise click.ClickException(
				f"Cannot install {file_path}: Missing data for {encoding_key}. "
				"Try running `ngdp fetch {remote}`."
			)

	bar.close()

	if errors:
		tqdm.write(f"{errors} files could not be written.")
		exit(1)


@main.command("log")
@click.argument("remote", type=RemoteParam())
@click.option("--type", default="versions", type=click.Choice(["versions", "cdns"]))
@click.pass_context
def show_log(ctx, remote, type):
	column = f"/{type}"
	results = ctx.obj.keg.db.get_responses(remote=remote, path=column)

	last = ""
	for digest, timestamp in results:
		if digest == last:
			# Skip contiguous digests (always only show oldest)
			continue
		last = digest
		click.secho(f"{type} {digest}", fg="yellow")
		click.echo("Date: " + datetime.fromtimestamp(timestamp).isoformat() + "Z")
		click.echo(f"URL: {remote + column}\n")
		if ctx.obj.keg.state_cache.exists(type, digest):
			contents = ctx.obj.keg.state_cache.read(type, digest)
			table = ctx.obj.tabulate(psv.loads(contents))
			click.secho(table, fg="black", bold=True)
		else:
			click.secho("(not available)", fg="red")
		click.echo()


@main.command("show")
@click.argument("remote", type=RemoteParam())
@click.argument("object")
@click.pass_context
def show_object(ctx, remote, object):
	if object.lower() == "buildconfig":
		column = "BuildConfig"
	elif object.lower() == "cdnconfig":
		column = "CDNConfig"
	elif object.lower() == "productconfig":
		column = "ProductConfig"
	else:
		raise click.ClickException(f"Unknown object type: {object}")

	cursor = ctx.obj.keg.db.cursor()

	cursor.execute(f"""
		SELECT distinct("{column}")
		FROM versions
		WHERE
			remote = ? AND
			"{column}" != ''
	""", (remote, ))

	results = cursor.fetchall()
	if not results:
		raise click.ClickException(f"No known {column} for {remote}.")

	for res, in results:
		click.echo(f"{column}: {res}")


@main.command("fsck")
@click.option("--delete", is_flag=True)
@click.pass_context
def fsck(ctx, delete):
	def _get_objects(patterns):
		return sorted(
			f for pattern in patterns
			for f in iglob(os.path.join(ctx.obj.keg.path, pattern))
		)

	objects = _get_objects((
		"responses/bgdl/*/*/*",
		"responses/blob/*/*/*/*",
		"responses/blobs/*/*/*",
		"responses/cdns/*/*/*",
		"responses/versions/*/*/*",
		"objects/config/*/*/*",
		"objects/configs/data/*/*/*",
		"objects/data/*/*/*",
		"objects/patch/*/*/*",
	))

	fail = 0
	bar = tqdm(unit="object", ncols=0, leave=True, total=len(objects))
	deleted_str = " (deleted)" if delete else ""
	for path in objects:
		bar.update()
		base_path = path[len(ctx.obj.keg.path) + 1:]  # Strip the full path to .ngdp
		key = os.path.basename(path)
		bar.set_description(f"Checking objects: {key} ({base_path})")
		if key.endswith(".keg_temp"):
			tqdm.write(f"Dangling file: {path}{deleted_str}", sys.stderr)
			if delete:
				os.remove(path)
			continue

		is_data = base_path.startswith("objects/data/")
		if len(key) != 32 and not (is_data and key.endswith(".index")):
			if delete:
				os.remove(path)
			tqdm.write(f"Unknown file: {path}{deleted_str}", sys.stderr)
			continue

		with open(path, "rb") as f:
			try:
				if is_data:
					if path.endswith(".index"):
						key = key[:-len(".index")]
						ArchiveIndex(f.read(), key, verify=True)
					elif os.path.exists(path + ".index"):
						with open(path + ".index", "rb") as index_file:
							index = ArchiveIndex(index_file.read(), key, verify=False)
							for item_key, item_size, item_offset in index.items:
								f.seek(item_offset)
								blte.verify_blte_data(BytesIO(f.read(item_size)), item_key)

					else:
						blte.verify_blte_data(f, key)
				else:
					verify_data("object on disk", f.read(), key, verify=True)
			except IntegrityVerificationError as e:
				if delete:
					os.remove(path)
				tqdm.write(f"Integrity error: {path}{deleted_str}", sys.stderr)
				tqdm.write(str(e), sys.stderr)
				fail += 1
			except Exception as e:
				import traceback
				tqdm.write(f"Error while verifying {path}", sys.stderr)
				tqdm.write(traceback.format_exc(), sys.stderr)

	bar.set_description("Checking responses: Done.")
	bar.close()

	if fail:
		tqdm.write(f"{fail} bad objects!")
		exit(1)


@main.command("parse-index")
@click.argument("key_or_path")
@click.pass_context
def parse_index(ctx, key_or_path):
	def get_actual_path(path):
		if os.path.exists(path):
			return path

		if looks_like_md5(key_or_path):
			real_path = os.path.join(
				ctx.obj.keg.objects_path,
				"data",
				partition_hash(key_or_path) + ".index"
			)
			if os.path.exists(real_path):
				return real_path

	real_path = get_actual_path(key_or_path)
	if not real_path:
		raise click.ClickException(f"No such file or object: {key_or_path}")

	key = os.path.basename(os.path.splitext(real_path)[0])
	if not looks_like_md5(key):
		click.echo(f"WARNING: Invalid key name {repr(key)}", file=sys.stderr)

	with open(real_path, "rb") as f:
		data = f.read()

	archive_index = ArchiveIndex(data, key, verify=ctx.obj.verify)

	click.secho(
		ctx.obj.tabulate(archive_index.items, headers=("Key", "Size", "Offset")),
		fg="black", bold=True
	)


@main.command("parse-encoding")
@click.argument("paths", nargs=-1)
@click.pass_context
def parse_encoding(ctx, paths):
	for path in paths:
		with open(path, "rb") as f:
			data = f.read()

		if data[:4] == b"BLTE":
			data = blte.loads(data, "", verify=False)

		encoding_file = EncodingFile(data, "", verify=False)

		click.echo(ctx.obj.tabulate(
			encoding_file.encoding_keys,
			headers=("Encoded key", "Encoding spec")
		))

		content_keys = [(row[0], row[1][1], *row[1][0]) for row in encoding_file.content_keys]
		# Handle scenario with more than 1 content key
		extra_columns = max(len(r) for r in content_keys) - 2
		headers = ["Content key", "Size", "Encoded key"]
		headers += ["Encoded key (alt.)"] * extra_columns

		click.echo(ctx.obj.tabulate(content_keys, headers=headers))


@main.group()
def archive():
	pass


@archive.command("list")
@click.option("--remote", type=RemoteParam(), multiple=True)
@click.option("--show-available/--hide-available", default=True)
@click.option("--show-unavailable/--hide-unavailable", default=True)
@click.pass_context
def list_archives(ctx, remote, show_available, show_unavailable):
	cdn = ctx.obj.keg.local_cdn
	cdn_config_keys = ctx.obj.keg.db.get_cdn_configs(remotes=remote)
	archives = set()
	for cdn_config_key in cdn_config_keys:
		try:
			cdn_config = cdn.get_cdn_config(cdn_config_key)
		except FileNotFoundError:
			continue

		archives.update(cdn_config.archives)

	for archive_key in sorted(archives):
		if cdn.has_data(archive_key):
			if show_available:
				click.secho(archive_key, bold=True)
		else:
			if show_unavailable:
				click.secho(archive_key, fg="black", bold=True)


@archive.command("extract")
@click.argument("archive-key")
@click.option("--out-dir", default=".")
@click.option("--extract-blte/--no-extract-blte", default=True)
@click.pass_context
def extract_archive(ctx, archive_key, out_dir, extract_blte):
	cdn = ctx.obj.keg.local_cdn
	if not cdn.has_data(archive_key):
		raise click.ClickException(f"Archive {archive_key} not found.")
	if not cdn.has_index(archive_key):
		raise click.ClickException(f"Archive index {archive_key} not found.")

	out_dir = os.path.abspath(os.path.join(out_dir, archive_key))
	if os.path.exists(out_dir):
		raise click.ClickException(f"Directory {out_dir} already exists")
	os.makedirs(out_dir)

	archive = cdn.get_archive(archive_key)
	archive_index = cdn.get_index(archive_key, verify=ctx.obj.verify)
	items = list(archive_index.items)

	bar = tqdm(unit="files", ncols=0, leave=False, total=len(items))
	for key, size, offset in items:
		bar.update()
		bar.set_description(f"Extracting {key}")
		if extract_blte:
			file_data = archive.get_file(key, size, offset, verify=ctx.obj.verify)
		else:
			file_data = archive.get_file_data(size, offset)

		with open(os.path.join(out_dir, key), "wb") as f:
			f.write(file_data)

	bar.close()


@archive.command("create")
@click.argument("archive-key")
@click.argument("fragment-paths", nargs=-1)
@click.pass_context
def reconstruct_archive(ctx, archive_key, fragment_paths):
	cdn = ctx.obj.keg.local_cdn
	if cdn.has_data(archive_key):
		raise click.ClickException(f"Archive {archive_key} already exists.")
	if not cdn.has_index(archive_key):
		raise click.ClickException(f"Index for {archive_key} not found.")

	archive_index = cdn.get_index(archive_key, verify=ctx.obj.verify)
	items = list(archive_index.items)
	click.echo(f"Reconstructing archive: {len(items)} items required.")

	# Sort items by offset, so they get written serially without seeking
	items.sort(key=lambda k: k[2])

	errors = 0
	temp_path = f"{archive_key}.keg_temp"
	with open(temp_path, "wb") as archive_file:
		bar = tqdm(unit="fragment", ncols=0, leave=False, total=len(items))
		for key, size, offset in items:
			bar.update()
			for path in fragment_paths:
				filename = os.path.join(path, key)
				if not os.path.exists(filename):
					continue
				with open(filename, "rb") as f:
					if ctx.obj.verify:
						try:
							blte.verify_blte_data(f, key)
						except blte.BLTEError as e:
							# This is actually okay, we only write the required bytes.
							tqdm.write(f"WARNING: {str(e)}")
						f.seek(0)
					# Make sure we didn't miss some data somehow
					assert archive_file.tell() == offset
					archive_file.write(f.read(size))
				break
			else:
				errors += 1
				tqdm.write(f"ERROR: Could not find fragment: {key}", sys.stderr)
				# Write nulls to skip ahead
				archive_file.write(b"\0" * size)
	bar.close()

	if errors:
		click.echo(
			f"Reconstruction failed: {errors}/{len(items)} bad or missing fragments.",
			sys.stderr
		)
		os.remove(temp_path)
	else:
		os.rename(temp_path, archive_key)
		click.echo(f"Written to {os.path.abspath(archive_key)}")


if __name__ == "__main__":
	main()
